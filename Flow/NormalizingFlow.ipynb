{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Normalizing Flow\n",
    "## 1.Normalizing Flow\n",
    "## 2.RealNVP\n",
    "## 3.Glow\n",
    "## 4.Autoregressive Flows\n",
    "## 5.VAE+Flows"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "837762229b92966c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Normalizing Flow\n",
    "## 1. 引言\n",
    "\n",
    "- **背景介绍**  \n",
    "  - 生成模型旨在学习数据分布，并能够生成与真实数据相似的样本。  \n",
    "  - Flow-based 模型是一类生成模型，通过一系列可逆变换将简单分布（如标准高斯分布）映射到复杂的数据分布。  \n",
    "  - 与 GAN 和 VAE 相比，Flow-based 模型的优势在于能够精确计算对数似然，因此可以采用最大似然估计进行训练。\n",
    "\n",
    "- **基本思想**  \n",
    "  - 利用**可逆变换**（Invertible Transformation）逐步变换分布。  \n",
    "  - 通过**变量替换定理**（Change of Variables Theorem）计算概率密度的变化。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Normalizing Flow 的推导\n",
    "\n",
    "### 2.1 变量替换定理\n",
    "\n",
    "- **一维情形**  \n",
    "  假设 $\\( z \\)$ 服从简单分布 $\\( p_Z(z) \\)$，通过一对一可逆变换 $\\( x = f(z) \\)$ 得到 $\\( x \\)$ 的分布。  \n",
    "  根据变量替换定理，$\\( x \\)$ 的密度为：\n",
    "  $\\[\n",
    "  p_X(x) = p_Z(z) \\left|\\frac{dz}{dx}\\right| = p_Z(f^{-1}(x)) \\left|\\frac{d}{dx} f^{-1}(x)\\right|\n",
    "  \\]$\n",
    "\n",
    "- **多维情形**  \n",
    "  若 $\\( z \\in \\mathbb{R}^D \\)$，并有变换 $\\( x = f(z) \\)$，则：\n",
    "  $\\[\n",
    "  p_X(x) = p_Z(z) \\left|\\det \\frac{\\partial f^{-1}(x)}{\\partial x}\\right| = p_Z(f^{-1}(x)) \\left|\\det \\frac{\\partial f^{-1}(x)}{\\partial x}\\right|\n",
    "  \\]$\n",
    "  其中 $\\(\\frac{\\partial f^{-1}(x)}{\\partial x}\\)$ 是雅可比矩阵，$\\(\\det\\)$ 表示其行列式。\n",
    "\n",
    "### 2.2 连续变换与多步变换\n",
    "\n",
    "- **离散多个变换**  \n",
    "  假设我们有一系列可逆变换 $\\( f_1, f_2, \\dots, f_K \\)$，令\n",
    "  $\\[\n",
    "  x = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(z)\n",
    "  \\]$\n",
    "  则整个变换下的密度为：\n",
    "  $\\[\n",
    "  p_X(x) = p_Z(z) \\prod_{k=1}^{K} \\left|\\det \\frac{\\partial f_k^{-1}(x_k)}{\\partial x_k}\\right|\n",
    "  \\]$\n",
    "  其中 $\\( x_k = f_k(x_{k-1}) \\)$ 且 $\\( x_0 = z \\)$。\n",
    "\n",
    "- **连续流模型（Continuous Normalizing Flows, CNF）**  \n",
    "  当变换层数趋于无穷多时，可看作隐藏状态在连续时间 $\\( t \\)$ 上演化：\n",
    "  $\\[\n",
    "  \\frac{dz(t)}{dt} = f(z(t), t, \\theta)\n",
    "  \\]$\n",
    "  初始时 $\\( z(0) \\sim p_Z(z) \\)$，最终 $\\( z(T) \\)$ 为生成样本。  \n",
    "  利用微积分，可以写出密度演化的公式：\n",
    "  $\\[\n",
    "  \\log p(z(T)) = \\log p(z(0)) - \\int_{0}^{T} \\text{Tr}\\left(\\frac{\\partial f}{\\partial z(t)}\\right) dt\n",
    "  \\]$\n",
    "  这里的 $\\(\\text{Tr}\\)$ 表示雅可比矩阵的迹。\n",
    "\n",
    "### 2.3 关键推导步骤总结\n",
    "\n",
    "1. 从简单分布出发，通过可逆变换得到目标分布。  \n",
    "2. 利用**变量替换定理**将原分布的密度转化为新分布的密度，关键在于计算雅可比行列式。  \n",
    "3. 多层变换中的密度计算通过**链式法则**实现：每一步的密度变化乘积给出最终密度。  \n",
    "4. 在连续时间下，密度变化由微分方程描述，积分项为雅可比迹的累积。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Normalizing Flow 的应用\n",
    "\n",
    "### 3.1 密度估计和生成模型\n",
    "\n",
    "- **最大似然训练**  \n",
    "  - Flow-based 模型能够直接计算 $\\( p(x) \\)$ 的精确对数似然。  \n",
    "  - 训练目标为最小化负对数似然（NLL）：\n",
    "    $\\[\n",
    "    \\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log p(x^{(i)})\n",
    "    \\]$\n",
    "\n",
    "- **生成新样本**  \n",
    "  - 生成时从简单分布 $\\( p_Z(z) \\)$ 中采样，再经过一系列可逆变换 $\\( x = f(z) \\)$ 得到生成样本。\n",
    "\n",
    "### 3.2 常见的 Normalizing Flow 模型\n",
    "\n",
    "- **Real NVP**  \n",
    "  - 采用耦合层（Coupling Layers）构建可逆变换，便于计算雅可比行列式（通常只涉及部分变量的变换）。\n",
    "  \n",
    "- **Glow**  \n",
    "  - 在 Real NVP 基础上改进，加入可逆 1×1 卷积操作，实现更灵活的数据映射。\n",
    "\n",
    "- **FFJORD**  \n",
    "  - 基于连续时间的 CNF，通过 Hutchinson’s 迹估计等技巧高效计算密度，适用于高维数据。\n",
    "\n",
    "### 3.3 应用场景\n",
    "\n",
    "- **图像生成**  \n",
    "  - Flow-based 模型能够生成高质量的图像，如 Glow 在生成真实感图像上的表现。\n",
    "  \n",
    "- **密度估计**  \n",
    "  - 对复杂数据分布进行精确建模，适用于异常检测、数据建模等任务。\n",
    "\n",
    "- **变分推断**  \n",
    "  - Flow-based 方法可以作为 VAE 的后验分布近似器，提升 VAE 的表达能力。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 总结与思考\n",
    "\n",
    "- **推导部分**：  \n",
    "  - 关键在于理解如何利用变量替换定理计算分布密度，并通过链式法则将多个可逆变换连接起来。  \n",
    "  - 连续时间下，积分形式的密度更新公式是核心，这需要对微分方程和雅可比迹有一定的理解。\n",
    "\n",
    "- **应用部分**：  \n",
    "  - Normalizing Flow 的一个重要优点是可以直接计算精确的对数似然，因此在生成模型中具有天然优势。  \n",
    "  - 不同模型（Real NVP、Glow、FFJORD）各有侧重，选择时需要结合具体任务和数据特点。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 参考资料与后续阅读\n",
    "\n",
    "- **原始论文**：\"Density Estimation using Real NVP\" (Dinh et al., 2016)  \n",
    "- **Glow**：\"Glow: Generative Flow with Invertible 1×1 Convolutions\" (Kingma et al., 2018)  \n",
    "- **Neural ODEs**（与连续流相关）：\"Neural Ordinary Differential Equations\" (Chen et al., 2018)  \n",
    "- **综述文章**：可以寻找关于 Flow-based Generative Models 的综述，帮助更系统地梳理各个模型的原理与差异。\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59f6183157a1d05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 模型结构中的深度神经网络部分\n",
    "\n",
    "在 Flow-based 模型中，我们需要构造一个或一系列**可逆变换**  \n",
    "这些变换的函数形式通常由深度神经网络来参数化，例如：\n",
    "$\\[\n",
    "x = f(z; \\theta)\n",
    "\\]$\n",
    "其中：  \n",
    "- $\\( z \\)$ 是从一个简单分布（如标准高斯）采样的变量；  \n",
    "- $\\( f(\\cdot; \\theta) \\)$ 是一个深度神经网络，参数为 $\\( \\theta \\)$；  \n",
    "- 通过这个变换，我们可以得到目标数据 $\\( x \\)$ 的分布。\n",
    "\n",
    "对于连续的变换（如 CNF），我们把变换过程看作一个连续时间过程，描述为常微分方程：\n",
    "$\\[\n",
    "\\frac{dz(t)}{dt} = f(z(t), t; \\theta)\n",
    "\\]$\n",
    "其中同样 $\\( f \\)$ 由深度神经网络实现，其参数 $\\( \\theta \\)$ 就是我们需要优化的。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 推导结果给出的密度计算公式\n",
    "\n",
    "### 离散多步变换情形\n",
    "\n",
    "假设我们有多个可逆变换 $\\( f_1, f_2, \\dots, f_K \\)$ 且它们参数均由神经网络控制，那么经过这 K 步变换后的密度根据变量替换定理可以写为：\n",
    "$\\[\n",
    "p_X(x) = p_Z(z) \\prod_{k=1}^{K} \\left|\\det \\frac{\\partial f_k^{-1}(x_k)}{\\partial x_k}\\right|\n",
    "\\]$\n",
    "其中每个 $\\( f_k \\)$ 都是由神经网络设计的。  \n",
    "这就意味着：  \n",
    "- **初始密度 $\\( p_Z(z) \\)$** 是已知的（例如标准正态分布），  \n",
    "- **每一层变换** 对应的雅可比行列式可以通过神经网络计算（或者设计成易于求雅可比行列式的形式，如耦合层）。\n",
    "\n",
    "### 连续变换情形\n",
    "\n",
    "在连续情形下，密度变化公式可以写为：\n",
    "$\\[\n",
    "\\log p(z(T)) = \\log p(z(0)) - \\int_{0}^{T} \\text{Tr}\\left(\\frac{\\partial f}{\\partial z(t)}\\right) dt\n",
    "\\]$\n",
    "这里的 $\\( f(z(t), t; \\theta) \\)$ 同样是由深度神经网络参数化的。  \n",
    "这个积分项表示在从 $\\( t=0 \\)$ 到 $\\( t=T \\)$ 过程中，密度的变化累积了雅可比矩阵迹的影响。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 如何优化神经网络参数？\n",
    "\n",
    "不论是离散多步还是连续变换，我们最终的目标是通过**最大似然估计**来优化模型参数 $\\( \\theta \\)$。具体步骤如下：\n",
    "\n",
    "### 3.1 定义损失函数\n",
    "\n",
    "通常我们使用负对数似然（Negative Log Likelihood, NLL）作为损失函数。  \n",
    "假设我们有训练数据 $\\( \\{x^{(i)}\\}_{i=1}^N \\)$，则损失函数为：\n",
    "$\\[\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_X(x^{(i)}; \\theta)\n",
    "\\]$\n",
    "其中 $\\( \\log p_X(x^{(i)}; \\theta) \\)$ 的计算依赖于前面推导的密度变换公式。  \n",
    "例如，对于离散多步变换：\n",
    "$\\[\n",
    "\\log p_X(x) = \\log p_Z(z) + \\sum_{k=1}^{K} \\log \\left|\\det \\frac{\\partial f_k^{-1}(x_k)}{\\partial x_k}\\right|\n",
    "\\]$\n",
    "而对于连续情形则涉及积分项。\n",
    "\n",
    "### 3.2 自动微分与反向传播\n",
    "\n",
    "由于整个变换 $\\( f \\)$ 以及密度计算过程都是**可微分的**，我们可以利用自动微分工具（如 PyTorch 或 TensorFlow）来计算损失函数关于参数 $\\( \\theta \\)$ 的梯度。  \n",
    "- 在离散情形中，每一层 $\\( f_k \\)$ 的神经网络都是普通的前向计算图，可以直接用反向传播计算梯度。\n",
    "- 在连续情形中，由于涉及 ODE 求解器，论文提出了**伴随方法（Adjoint Method）**来反向传播梯度。这同样是自动微分的一种形式，只不过它允许我们在不保存整个 ODE 求解过程的中间状态下计算梯度，从而降低内存消耗。\n",
    "\n",
    "### 3.3 参数更新\n",
    "\n",
    "有了损失函数和梯度之后，就可以使用梯度下降（例如 Adam 优化器）更新神经网络参数 $\\( \\theta \\)$：\n",
    "$\\[\n",
    "\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta)\n",
    "\\]$\n",
    "其中 $\\( \\eta \\)$ 是学习率。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 直观理解\n",
    "\n",
    "可以把整个流程想象为以下几个步骤：\n",
    "1. **输入数据 $\\( x \\)$ 与一个简单分布中的 $\\( z \\)$ 对应起来**（通过逆变换 $\\( f^{-1} \\)$）。\n",
    "2. **利用变量替换定理计算 $\\( \\log p(x) \\)$**，其中涉及神经网络参数化的各个变换的雅可比行列式。\n",
    "3. **构造损失函数**（负对数似然），目标是让模型生成的 $\\( p(x) \\)$ 尽可能接近真实数据分布。\n",
    "4. **利用自动微分和反向传播**，根据损失函数计算梯度，进而更新神经网络参数 $\\( \\theta \\)$。\n",
    "\n",
    "整个过程与其他深度学习模型训练类似，只不过在这里，损失函数中包含了通过一系列可逆变换（或连续 ODE 求解）得到的密度计算部分。关键是这些变换都是由深度神经网络参数化的，所以模型的表达能力和参数优化能力都依赖于深度学习的标准技术。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 小结\n",
    "\n",
    "- **推导结果**：通过一系列变换（离散多步或连续 ODE）得到了数据密度的表达式，该表达式依赖于神经网络参数 $\\( \\theta \\)$。\n",
    "- **损失函数**：利用负对数似然作为损失，将密度表达式代入其中。\n",
    "- **参数优化**：利用自动微分和反向传播（或伴随方法）计算梯度，然后用梯度下降方法更新参数。\n",
    "\n",
    "这样，深度神经网络的参数优化就与生成模型中的密度计算紧密结合起来了，从而使得整个 flow-based 生成模型可以端到端地训练。\n",
    "\n",
    "希望这份详细解释能帮你更好地理解如何将推导结果与深度神经网络参数优化结合起来！"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cfc24dcf4de038d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我详细展开如何构造离散多步变换下的损失函数，并说明其与深度神经网络参数优化的结合过程。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 模型描述\n",
    "\n",
    "假设我们设计一个 Flow-based 生成模型，通过一系列可逆变换将一个简单分布（通常是标准正态分布）映射到复杂数据分布。具体来说，我们假设：\n",
    "\n",
    "1. 从简单分布采样 latent 变量：\n",
    "   $\\[\n",
    "   z \\sim p_Z(z)\n",
    "   \\]$\n",
    "   例如 $\\( p_Z(z) = \\mathcal{N}(0, I) \\)$。\n",
    "\n",
    "2. 通过多个可逆变换生成样本 $\\( x \\)$：\n",
    "   $\\[\n",
    "   x = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(z)\n",
    "   \\]$\n",
    "   每个变换 $\\( f_k \\)$ 通常由深度神经网络参数化，参数记为 $\\( \\theta_k \\)$，整个模型参数记为 $\\( \\theta \\)$。\n",
    "\n",
    "我们也可以定义整体变换 $\\( f \\)$ 和其逆变换 $\\( f^{-1} \\)$：\n",
    "$\\[\n",
    "z = f^{-1}(x).\n",
    "\\]$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 变量替换定理和密度计算\n",
    "\n",
    "根据变量替换定理，若 $\\( x = f(z) \\)$ 且 $\\( f \\)$ 可逆，则\n",
    "$\\[\n",
    "p_X(x) = p_Z(z) \\left| \\det \\frac{\\partial f^{-1}(x)}{\\partial x} \\right|.\n",
    "\\]$\n",
    "\n",
    "对于多步变换来说，我们可以将整体逆变换拆分为每一步的逆变换。设：\n",
    "$\\[\n",
    "z_0 = z,\\quad z_1 = f_1(z_0),\\quad z_2 = f_2(z_1),\\quad \\dots,\\quad z_K = f_K(z_{K-1}) = x.\n",
    "\\]$\n",
    "那么整个 Jacobian 的行列式可以写成各步的雅可比行列式的乘积：\n",
    "$\\[\n",
    "\\left| \\det \\frac{\\partial f^{-1}(x)}{\\partial x} \\right| = \\prod_{k=1}^K \\left| \\det \\frac{\\partial f_k^{-1}(z_k)}{\\partial z_k} \\right|.\n",
    "\\]$\n",
    "或者用前向计算的方式（更常见）：\n",
    "$\\[\n",
    "p_X(x) = p_Z(z_0) \\prod_{k=1}^{K} \\left| \\det \\frac{\\partial f_k(z_{k-1})}{\\partial z_{k-1}} \\right|^{-1}.\n",
    "\\]$\n",
    "两种写法等价，只是侧重点不同。下面我们采用前向表达式来描述。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 构造对数似然\n",
    "\n",
    "对于单个训练样本 $\\( x \\)$，我们可以通过以下步骤计算其对数似然：\n",
    "\n",
    "1. **计算隐变量**：通过逆变换求出对应的 $\\( z_0 = f^{-1}(x) \\)$（在前向流程中，我们假设 $\\( z_0 \\)$ 已知，然后通过连续变换生成 $\\( x \\)$，训练时通过逆向传递可以计算）。\n",
    "   \n",
    "2. **利用先验密度**：计算 $\\( \\log p_Z(z_0) \\)$。例如，如果 $\\( p_Z(z) = \\mathcal{N}(0, I) \\)$，则\n",
    "   $\\[\n",
    "   \\log p_Z(z_0) = -\\frac{1}{2} \\|z_0\\|^2 + \\text{const.}\n",
    "   \\]$\n",
    "\n",
    "3. **累加各步的密度变化**：对于每一步 $\\( k=1,\\dots, K \\)$，计算神经网络对应的变换 $\\( f_k \\)$ 的 Jacobian 行列式，并累加其对数：\n",
    "   $\\[\n",
    "   \\sum_{k=1}^{K} \\log \\left| \\det \\frac{\\partial f_k(z_{k-1})}{\\partial z_{k-1}} \\right|.\n",
    "   \\]$\n",
    "   因为前向公式中密度会被除以这些因子，所以整体对数似然为：\n",
    "   $\\[\n",
    "   \\log p_X(x) = \\log p_Z(z_0) - \\sum_{k=1}^{K} \\log \\left| \\det \\frac{\\partial f_k(z_{k-1})}{\\partial z_{k-1}} \\right|.\n",
    "   \\]$\n",
    "\n",
    "注意：在实际实现中，为了计算效率，通常设计变换 $\\( f_k \\)$ 的结构（例如耦合层、稀疏矩阵或三角形结构），使得雅可比行列式的计算可以简化为只涉及部分变量或简单函数。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 损失函数构造\n",
    "\n",
    "假设我们有 $\\( N \\)$ 个训练样本 $\\( \\{x^{(i)}\\}_{i=1}^{N} \\)$。对于每个样本 $\\( x^{(i)} \\)$，按照上面的步骤计算对数似然：\n",
    "$\\[\n",
    "\\log p_X(x^{(i)}) = \\log p_Z(z_0^{(i)}) - \\sum_{k=1}^{K} \\log \\left| \\det \\frac{\\partial f_k(z_{k-1}^{(i)})}{\\partial z_{k-1}^{(i)}} \\right|.\n",
    "\\]$\n",
    "\n",
    "然后，定义负对数似然损失函数（NLL）为：\n",
    "$\\[\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_X(x^{(i)}).\n",
    "\\]$\n",
    "将上式展开：\n",
    "$\\[\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\log p_Z(z_0^{(i)}) - \\sum_{k=1}^{K} \\log \\left| \\det \\frac{\\partial f_k(z_{k-1}^{(i)})}{\\partial z_{k-1}^{(i)}} \\right| \\right].\n",
    "\\]$\n",
    "或者写为：\n",
    "$\\[\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_Z(z_0^{(i)}) + \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\log \\left| \\det \\frac{\\partial f_k(z_{k-1}^{(i)})}{\\partial z_{k-1}^{(i)}} \\right|.\n",
    "\\]$\n",
    "\n",
    "这就是流模型的训练目标：通过最小化负对数似然，使得模型生成的样本分布 $\\( p_X(x) \\)$ 与训练数据分布尽可能接近。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 如何与神经网络参数优化结合？\n",
    "\n",
    "1. **参数化变换**：  \n",
    "   每个变换 $\\( f_k \\)$ 是由深度神经网络参数化的，其参数记为 $\\( \\theta_k \\)$（整体记为 $\\( \\theta \\)$）。因此，上述表达式中的雅可比行列式和计算 $\\( f_k(z_{k-1}) \\)$ 都依赖于 $\\( \\theta \\)$。\n",
    "\n",
    "2. **自动微分**：  \n",
    "   使用像 PyTorch、TensorFlow 这样的深度学习框架，可以自动计算损失函数 $\\( \\mathcal{L}(\\theta) \\)$ 关于 $\\( \\theta \\)$ 的梯度。由于所有步骤（包括神经网络前向计算和雅可比行列式的计算，若设计得当）都是可微分的，框架会自动构建计算图并计算梯度。\n",
    "\n",
    "3. **梯度下降更新**：  \n",
    "   利用计算得到的梯度，采用梯度下降（如 Adam 优化器）更新参数：\n",
    "   $\\[\n",
    "   \\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta),\n",
    "   \\]$\n",
    "   其中 $\\( \\eta \\)$ 是学习率。\n",
    "\n",
    "4. **端到端训练**：  \n",
    "   整个流程（从 $\\( z \\)$ 的采样到通过一系列变换得到 $\\( x \\)$ 的密度计算，再到损失函数的求和）构成一个端到端的模型。优化器会不断调整神经网络的参数，使得计算出的对数似然最大化（或负对数似然最小化），从而训练出一个能够生成数据的流模型。\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 直观理解\n",
    "\n",
    "可以将整个过程想象为以下步骤：\n",
    "- **采样**：先从一个简单的分布中采样出 $\\( z \\)$。\n",
    "- **变换**：通过多个神经网络变换 $\\( f_1, f_2, \\dots, f_K \\)$ 将 $\\( z \\)$ 变换为 $\\( x \\)$。\n",
    "- **密度计算**：利用公式计算 $\\( x \\)$ 的密度，其中包含先验密度和每一步变换引入的“体积变化”。\n",
    "- **损失构造**：对每个训练样本计算负对数似然，然后取平均。\n",
    "- **参数更新**：利用反向传播自动计算梯度，然后更新神经网络参数，使得生成的 $\\( x \\)$ 分布更接近真实数据分布。\n",
    "\n",
    "---\n",
    "\n",
    "## 7. 小结\n",
    "\n",
    "- **离散多步变换**中，每个训练样本 $\\( x^{(i)} \\)$ 的对数似然计算为：\n",
    "  $\\[\n",
    "  \\log p_X(x^{(i)}) = \\log p_Z(z^{(i)}) - \\sum_{k=1}^{K} \\log \\left|\\det \\frac{\\partial f_k(z_{k-1}^{(i)})}{\\partial z_{k-1}^{(i)}}\\right|.\n",
    "  \\]$\n",
    "- **损失函数**为所有样本的负平均对数似然：\n",
    "  $\\[\n",
    "  \\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p_X(x^{(i)}).\n",
    "  \\]$\n",
    "- 利用自动微分，计算损失关于模型参数 $\\( \\theta \\)$ 的梯度，然后用梯度下降法更新参数，实现端到端的训练。\n",
    "\n",
    "通过这种方式，我们将理论推导（变量替换定理、Jacobian 计算、密度变化）与神经网络的前向传播和反向传播结合起来，实现了流模型的训练。\n",
    "\n",
    "希望这个详细的解释能帮助你理解如何构造损失函数以及如何利用深度学习工具进行参数优化！"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fdea3c07dc9175f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.Real NVP\n",
    "下面我详细介绍一下 **Real NVP（Real-valued Non-Volume Preserving）** 的核心思想、数学原理、架构设计以及如何实现它。Real NVP 是一种流式生成模型（Flow-based Generative Model），其主要优势在于：\n",
    "\n",
    "- 能够**精确计算对数似然**，从而可以用最大似然估计训练模型；\n",
    "- 模型设计保证了**可逆性**，使得采样和密度估计都很高效；\n",
    "- 通过设计巧妙的变换结构（例如耦合层 Coupling Layers），可以简化雅可比行列式（Jacobian determinant）的计算，使得计算复杂度低。\n",
    "\n",
    "下面我们从多个方面详细说明 Real NVP。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 基本思想\n",
    "\n",
    "Real NVP 的目标是将一个复杂的数据分布 \\( p_X(x) \\) 映射到一个简单的先验分布 \\( p_Z(z) \\)（通常选取标准正态分布 \\( \\mathcal{N}(0, I) \\)）上。整个映射是通过一系列可逆变换 \\( f = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1 \\) 实现的，即\n",
    "\n",
    "\\[\n",
    "x \\overset{f}{\\longrightarrow} z, \\quad \\text{且} \\quad z = f(x).\n",
    "\\]\n",
    "\n",
    "由于变换是可逆的，我们可以写出变量替换定理（Change of Variables）的公式来计算数据 \\( x \\) 的密度：\n",
    "\n",
    "\\[\n",
    "p_X(x) = p_Z(z) \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|.\n",
    "\\]\n",
    "\n",
    "而在训练中我们希望最大化 \\( \\log p_X(x) \\) 或最小化负对数似然（Negative Log Likelihood, NLL）。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 关键技术：耦合层（Coupling Layers）\n",
    "\n",
    "直接设计一个任意复杂的可逆变换，其雅可比行列式通常计算量非常大。Real NVP 的核心贡献在于提出了一种**耦合层结构**，该结构设计为满足以下两个条件：\n",
    "\n",
    "1. **可逆性**：每一层都必须是可逆的，这样才能在生成和密度计算时得到一致的映射。\n",
    "2. **雅可比行列式易计算**：变换的雅可比矩阵设计为下三角或上三角结构，其行列式仅为对角线上元素的乘积，计算复杂度由 \\( O(D^2) \\) 降为 \\( O(D) \\)（其中 \\( D \\) 是数据维度）。\n",
    "\n",
    "### 2.1 分割输入\n",
    "\n",
    "通常，我们将输入 \\( x \\) 分成两部分，假设 \\( x = (x_a, x_b) \\)。常见的做法是交替进行分割（例如，对于二维数据，可以用掩码 \\( m = [1, 0] \\) 或 \\( m = [0, 1] \\)）。\n",
    "\n",
    "### 2.2 仿射耦合层（Affine Coupling Layer）\n",
    "\n",
    "在 Real NVP 中，一个常用的耦合层的设计如下：  \n",
    "- 对于 \\( x = (x_a, x_b) \\)，令其中一部分（例如 \\( x_a \\)）保持不变，另一部分 \\( x_b \\) 则经过仿射变换，其参数由 \\( x_a \\) 决定：\n",
    "  \n",
    "  \\[\n",
    "  \\begin{aligned}\n",
    "  y_a &= x_a, \\\\\n",
    "  y_b &= x_b \\odot \\exp(s(x_a)) + t(x_a),\n",
    "  \\end{aligned}\n",
    "  \\]\n",
    "  \n",
    "  其中：\n",
    "  - \\( s(x_a) \\) 和 \\( t(x_a) \\) 分别是由神经网络生成的缩放（scale）和平移（translation）参数；\n",
    "  - “\\(\\odot\\)” 表示逐元素相乘；\n",
    "  - 指数 \\( \\exp(s(x_a)) \\) 保证变换是正定的，从而保证可逆性。\n",
    "\n",
    "- **逆变换**也非常简单，由于 \\( x_a = y_a \\) 保持不变，有\n",
    "  \n",
    "  \\[\n",
    "  \\begin{aligned}\n",
    "  x_a &= y_a, \\\\\n",
    "  x_b &= \\big(y_b - t(y_a)\\big) \\odot \\exp\\big(-s(y_a)\\big).\n",
    "  \\end{aligned}\n",
    "  \\]\n",
    "\n",
    "### 2.3 雅可比行列式计算\n",
    "\n",
    "对于上述仿射耦合层，其变换的雅可比矩阵 \\( J \\) 是分块下三角的：\n",
    "  \n",
    "\\[\n",
    "J = \\begin{pmatrix}\n",
    "I & 0 \\\\\n",
    "\\frac{\\partial y_b}{\\partial x_a} & \\text{diag}\\big(\\exp(s(x_a))\\big)\n",
    "\\end{pmatrix}.\n",
    "\\]\n",
    "\n",
    "由于行列式满足：\n",
    "  \n",
    "\\[\n",
    "\\det J = \\det(I) \\times \\det\\big(\\text{diag}(\\exp(s(x_a)))\\big) = \\prod_{i} \\exp(s_i(x_a)),\n",
    "\\]\n",
    "  \n",
    "所以取对数后：\n",
    "  \n",
    "\\[\n",
    "\\log \\left|\\det J\\right| = \\sum_{i} s_i(x_a).\n",
    "\\]\n",
    "\n",
    "这使得计算对数似然时，只需计算神经网络输出的 \\( s(x_a) \\) 的和，非常高效。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 训练目标和负对数似然\n",
    "\n",
    "假设我们有一个样本 \\( x \\) 经过若干层耦合层后，映射到 latent 空间 \\( z \\) 上。整个变换为 \\( z = f(x) \\)，那么根据变量替换定理，对数似然为\n",
    "\n",
    "\\[\n",
    "\\log p_X(x) = \\log p_Z(z) + \\sum_{k=1}^{K} \\log \\left|\\det J_k\\right|,\n",
    "\\]\n",
    "\n",
    "其中 \\( J_k \\) 是第 \\( k \\) 层的雅可比矩阵，而对于仿射耦合层，上式中 \\( \\log |\\det J_k| \\) 就是该层输出的 \\( s(x_a) \\) 的和。\n",
    "\n",
    "训练时，我们最小化负对数似然（NLL）：\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\log p_X(x^{(i)}).\n",
    "\\]\n",
    "\n",
    "常见的先验分布 \\( p_Z(z) \\) 选择标准正态分布 \\( \\mathcal{N}(0,I) \\)，其对数密度计算为\n",
    "\n",
    "\\[\n",
    "\\log p_Z(z) = -\\frac{1}{2} \\|z\\|^2 + \\text{const.}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 模型架构和交替掩码设计\n",
    "\n",
    "为了使每一部分都能被更新，Real NVP 模型通常设计多个耦合层，并采用交替掩码的方式，例如：\n",
    "- 第一层：掩码 \\( m = [1, 0, 1, 0, \\dots] \\)，固定一部分变量，变换另一部分。\n",
    "- 第二层：采用互补的掩码 \\( m = [0, 1, 0, 1, \\dots] \\)。\n",
    "  \n",
    "如此堆叠多个层，可以确保所有维度的信息都能参与变换和更新。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 实现思路\n",
    "\n",
    "在 PyTorch 中实现 Real NVP 通常包括以下几个步骤：\n",
    "1. **构造网络模块**：设计用于生成 \\( s(x_a) \\) 和 \\( t(x_a) \\) 的神经网络（通常为多层感知机 MLP）。\n",
    "2. **实现仿射耦合层**：根据上面公式实现前向（计算 \\( y \\) 和 log-det）和逆向（生成 \\( x \\)）函数。\n",
    "3. **堆叠多个耦合层**：利用交替掩码将多个耦合层组合成整体流模型。\n",
    "4. **计算密度和训练**：对每个样本，通过正向传播计算 \\( z \\) 和累计的 log-det，结合先验 \\( p_Z(z) \\) 得到 \\( \\log p_X(x) \\)，构造负对数似然损失，并利用反向传播优化所有层的参数。\n",
    "\n",
    "前面我已经给出了一个简化的 PyTorch 实现示例，其中使用了 Affine Coupling 层和交替掩码来构建一个基本的 Flow 模型。Real NVP 正是采用类似的方法。\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "- **Real NVP** 利用一系列可逆的仿射耦合层，将简单分布（如高斯分布）通过连续、可逆变换映射到复杂数据分布上。\n",
    "- **关键设计**：利用掩码分割输入，固定一部分变量，仅对另一部分进行仿射变换，从而保证变换的可逆性与雅可比行列式计算的高效性。\n",
    "- **训练目标**：通过最大似然训练（最小化负对数似然），使得生成的 \\( p_X(x) \\) 与数据分布相匹配，同时可直接进行采样和精确密度估计。\n",
    "- **优点**：能够精确计算对数似然、采样速度快，且具有较高的生成质量；  \n",
    "  **缺点**：模型设计与网络架构需要精心调试，且对高维数据可能需要设计更复杂的耦合层和其他辅助结构。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca9ec3bda003c12"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "下面给出一个简单的 PyTorch 实现示例，该示例展示了如何利用离散多步变换构造一个 Flow-based 模型（这里以 RealNVP 的一种简化版本为例），以及如何利用负对数似然（NLL）损失训练模型。示例代码中定义了一个 affine coupling 层，该层由神经网络参数化，并计算相应的雅可比行列式，从而构造出损失函数。你可以在此基础上扩展和改进模型。\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 定义辅助神经网络：用于生成 scale 和 translation 参数\n",
    "# -------------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, out_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 定义 Affine Coupling 层（RealNVP 中常用）\n",
    "# -------------------------------\n",
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, mask):\n",
    "        \"\"\"\n",
    "        :param in_features: 输入维度（例如2或更高）\n",
    "        :param hidden_features: 隐藏层节点数\n",
    "        :param mask: 一个与输入维度相同的 0/1 张量，用于决定哪些部分保持不变\n",
    "        \"\"\"\n",
    "        super(AffineCoupling, self).__init__()\n",
    "        self.register_buffer('mask', mask)  # mask 固定不训练\n",
    "        \n",
    "        # 构造两个网络，分别生成 scale 和 translation 参数\n",
    "        # 注意：这里只对未被 mask 掩盖的部分进行变换\n",
    "        self.scale_net = FeedForward(in_features, hidden_features, in_features)\n",
    "        self.translate_net = FeedForward(in_features, hidden_features, in_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x 的形状：(batch_size, in_features)\n",
    "        x_masked = x * self.mask  # 掩盖部分保持不变\n",
    "        # 对未掩盖部分生成 scale 和 translation 参数\n",
    "        s = self.scale_net(x_masked) * (1 - self.mask)\n",
    "        t = self.translate_net(x_masked) * (1 - self.mask)\n",
    "        # 进行仿射变换\n",
    "        y = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
    "        # 计算这一层的 log-det-Jacobian，\n",
    "        # 对于仿射变换，Jacobian 的对角元素为 exp(s)，因此对数行列式为 sum(s)\n",
    "        log_det = torch.sum(s, dim=1)\n",
    "        return y, log_det\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        # 逆变换时需要根据掩盖部分重新计算 scale 和 translation 参数\n",
    "        y_masked = y * self.mask\n",
    "        s = self.scale_net(y_masked) * (1 - self.mask)\n",
    "        t = self.translate_net(y_masked) * (1 - self.mask)\n",
    "        # 求逆：先减 t，再除以 exp(s)\n",
    "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n",
    "        # 逆变换时 log_det 是负的\n",
    "        log_det = -torch.sum(s, dim=1)\n",
    "        return x, log_det\n",
    "\n",
    "# -------------------------------\n",
    "# 3. 定义 Normalizing Flow 模型（由多个 coupling 层构成）\n",
    "# -------------------------------\n",
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self, flow_layers):\n",
    "        \"\"\"\n",
    "        :param flow_layers: 一个 nn.ModuleList 或 list，包含多个可逆变换层\n",
    "        \"\"\"\n",
    "        super(NormalizingFlow, self).__init__()\n",
    "        self.flow_layers = nn.ModuleList(flow_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        log_det_total = 0\n",
    "        # 将输入 x 逐步变换，并累加各层的 log-det\n",
    "        for layer in self.flow_layers:\n",
    "            x, log_det = layer.forward(x)\n",
    "            log_det_total += log_det\n",
    "        return x, log_det_total\n",
    "    \n",
    "    def inverse(self, z):\n",
    "        log_det_total = 0\n",
    "        # 逆向依次应用每一层的逆变换\n",
    "        for layer in reversed(self.flow_layers):\n",
    "            z, log_det = layer.inverse(z)\n",
    "            log_det_total += log_det\n",
    "        return z, log_det_total\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 定义一个简单的 Normalizing Flow 模型实例\n",
    "# -------------------------------\n",
    "# 假设我们的数据是二维的\n",
    "D = 2  \n",
    "hidden_features = 128\n",
    "\n",
    "# 定义两个掩码：交替掩盖不同的维度（如：[1, 0] 和 [0, 1]）\n",
    "mask1 = torch.tensor([1.0, 0.0])\n",
    "mask2 = torch.tensor([0.0, 1.0])\n",
    "\n",
    "# 构造多个 coupling 层\n",
    "flow_layers = [\n",
    "    AffineCoupling(in_features=D, hidden_features=hidden_features, mask=mask1),\n",
    "    AffineCoupling(in_features=D, hidden_features=hidden_features, mask=mask2)\n",
    "]\n",
    "\n",
    "# 创建 flow 模型\n",
    "flow_model = NormalizingFlow(flow_layers)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. 定义先验分布：通常选择标准正态分布\n",
    "# -------------------------------\n",
    "def log_prob_standard_normal(z):\n",
    "    # 标准正态分布的对数密度\n",
    "    return -0.5 * torch.sum(z ** 2 + torch.log(2 * torch.pi * torch.ones_like(z)), dim=1)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. 训练过程（构造损失函数：负对数似然 NLL）\n",
    "# -------------------------------\n",
    "optimizer = optim.Adam(flow_model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 这里以随机数据为例，实际中应使用真实数据集\n",
    "    # 假设训练样本 x 的形状为 (batch_size, D)\n",
    "    x = torch.randn(batch_size, D)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 前向传播：将 x 映射到 latent 空间 z，同时得到累积的 log-det\n",
    "    z, log_det = flow_model.forward(x)\n",
    "    # 计算先验分布 log p(z)\n",
    "    log_pz = log_prob_standard_normal(z)\n",
    "    # 根据变量替换公式计算 log p(x)\n",
    "    log_px = log_pz + log_det  # 这里 log_det 实际上是对各层 log|det J| 的累加\n",
    "    # 损失函数：负对数似然\n",
    "    loss = -torch.mean(log_px)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d912d26f718c6bc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.GLOW\n",
    "\n",
    "下面我将详细介绍一下 Glow 模型，包括它的核心思想、主要架构组件、与之前方法的区别以及实现细节，帮助你更深入地理解这篇论文。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1. 背景与核心思想\n",
    "\n",
    "Glow 是由 Kingma 等人在 2018 年提出的一种流式生成模型，全名为 **\"Glow: Generative Flow with Invertible 1×1 Convolutions\"**。它继承了 Normalizing Flow 模型的思想，通过一系列可逆变换将简单分布（如标准正态分布）映射到复杂的数据分布，从而可以直接计算对数似然并实现高质量采样。\n",
    "\n",
    "Glow 的主要创新点包括：\n",
    "\n",
    "- **引入可逆 1×1 卷积（Invertible 1×1 Convolutions）**：相较于 Real NVP 中常用的固定或随机排列（permutation）操作，Glow 采用了一种可学习的、可逆的 1×1 卷积作为通道的混合方法，使得变换更加灵活，同时学习数据的局部通道相关性。\n",
    "- **Actnorm 层**：在 Glow 中，采用了 Actnorm 层来对数据进行归一化（类似于 BatchNorm，但不依赖于批次统计），使得训练更稳定。\n",
    "- **多尺度架构**：通过逐步“拆分”数据，Glow 采用多尺度（multi-scale）架构，有助于高效建模和生成高分辨率图像。\n",
    "\n",
    "总体来说，Glow 在生成模型中提出了一种结构简单而又高效的方式，通过引入可逆 1×1 卷积使得模型具有更强的表达能力和灵活性，同时保持了精确的对数似然计算能力。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2. Glow 的主要架构组件\n",
    "\n",
    "Glow 模型主要由以下几个关键模块构成：\n",
    "\n",
    "### 3.2.1 Actnorm 层\n",
    "\n",
    "- **作用**：对每个通道进行数据归一化。\n",
    "- **特点**：  \n",
    "  - 与 BatchNorm 不同，Actnorm 不依赖于批次统计数据，而是在模型初始化时利用一个批次数据来设定缩放和平移参数（scale 和 bias），随后这些参数作为可学习的参数参与训练。  \n",
    "  - 这种设计可以在小批量或单样本情况下保持稳定性。\n",
    "\n",
    "### 3.2.2 可逆 1×1 卷积（Invertible 1×1 Convolution）\n",
    "\n",
    "- **基本概念**：  \n",
    "  - 在 Flow-based 模型中，对数据通道进行排列（permutation）可以增加变换的灵活性。在 Real NVP 中，常用固定排列或随机排列来实现这一点；而 Glow 则提出了**可逆 1×1 卷积**，将这一过程变为一个可学习的操作。\n",
    "- **实现方式**：  \n",
    "  - 假设输入特征图的通道数为 \\(C\\)，可逆 1×1 卷积实际上对每个像素位置进行线性变换：\n",
    "    \\[\n",
    "    y = W x,\n",
    "    \\]\n",
    "    其中 \\(W\\) 是一个 \\(C \\times C\\) 的可训练矩阵。  \n",
    "  - 为了保证变换可逆，要求 \\(W\\) 是可逆矩阵。\n",
    "- **计算雅可比行列式**：  \n",
    "  - 对于每个空间位置，线性变换的雅可比行列式就是 \\( \\det(W) \\)。  \n",
    "  - 如果图像有 \\(H \\times W\\) 个位置，则整个卷积层的对数行列式为：\n",
    "    \\[\n",
    "    \\log \\left|\\det J\\right| = H \\times W \\times \\log |\\det(W)|.\n",
    "    \\]\n",
    "  - 在训练时可以直接通过矩阵的行列式来计算这一项，并将其加到整体对数似然中。\n",
    "\n",
    "### 3.2.3 仿射耦合层（Affine Coupling Layers）\n",
    "\n",
    "- **作用**：  \n",
    "  - 这一层与 Real NVP 中类似，将输入分为两部分，其中一部分保持不变，另一部分经过由前一部分条件下的仿射变换。\n",
    "- **公式**：  \n",
    "  - 假设输入 \\(x = (x_a, x_b)\\)，仿射变换为：\n",
    "    \\[\n",
    "    \\begin{aligned}\n",
    "    y_a &= x_a, \\\\\n",
    "    y_b &= x_b \\odot \\exp\\big(s(x_a)\\big) + t(x_a),\n",
    "    \\end{aligned}\n",
    "    \\]\n",
    "    其中 \\(s(x_a)\\) 和 \\(t(x_a)\\) 由神经网络（通常为多层感知机）计算得到；“\\(\\odot\\)” 表示逐元素相乘。\n",
    "- **可逆性**：  \n",
    "  - 逆变换简单，且雅可比行列式仅由 \\( \\exp(s(x_a)) \\) 组成，使得其对数行列式为 \\(\\sum s(x_a)\\)，计算非常高效。\n",
    "\n",
    "### 3.2.4 多尺度架构\n",
    "\n",
    "- **思路**：  \n",
    "  - 为了更高效地建模高分辨率图像，Glow 在层与层之间采用了“拆分”操作，将部分通道直接输出作为模型的最终输出，其余通道继续经过后续的耦合层处理。\n",
    "- **好处**：  \n",
    "  - 这种设计可以降低计算量，同时在生成时能分层次产生图像细节。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3. Glow 模型的训练与生成\n",
    "\n",
    "### 3.3.1 训练目标\n",
    "\n",
    "Glow 与其他 Flow-based 模型一样，利用最大似然估计进行训练。  \n",
    "- **正向过程**：  \n",
    "  - 从数据 \\( x \\) 经过一系列变换得到 latent 变量 \\( z \\)（同时累计每一步的 log-det，即对数雅可比行列式）。\n",
    "  - 根据变量替换公式，数据的对数似然为：\n",
    "    \\[\n",
    "    \\log p_X(x) = \\log p_Z(z) + \\sum_{i} \\log \\left| \\det J_i \\right|,\n",
    "    \\]\n",
    "    其中 \\( p_Z(z) \\) 通常取标准正态分布。\n",
    "- **损失函数**：  \n",
    "  - 负对数似然（NLL）：\n",
    "    \\[\n",
    "    \\mathcal{L} = -\\mathbb{E}_{x \\sim \\text{data}} \\big[ \\log p_X(x) \\big].\n",
    "    \\]\n",
    "- **优化**：  \n",
    "  - 利用自动微分和梯度下降（例如 Adam 优化器）更新所有参数（包括 actnorm 参数、可逆 1×1 卷积矩阵、耦合层中的神经网络参数）。\n",
    "\n",
    "### 3.3.2 生成过程\n",
    "\n",
    "- **采样**：  \n",
    "  - 从先验分布 \\( p_Z(z) \\) 中采样 \\( z \\)（例如标准正态分布）。\n",
    "- **逆变换**：  \n",
    "  - 将采样得到的 \\( z \\) 按照逆变换依次传入模型（逆向的耦合层、逆向的 1×1 卷积、逆向的 Actnorm 等），最终得到生成样本 \\( x \\)。\n",
    "- **效果**：  \n",
    "  - 由于模型在训练时直接最大化对数似然，生成出的图像通常具有很好的细节和多样性。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4. Glow 与 Real NVP 的区别\n",
    "\n",
    "虽然 Glow 与 Real NVP 都属于 Flow-based 模型，但它们在以下几个方面有所不同：\n",
    "\n",
    "- **排列操作**：  \n",
    "  - Real NVP 常采用固定的反转或随机排列来交换通道；  \n",
    "  - Glow 则引入了可逆 1×1 卷积，使得通道混合成为一个可学习的过程，提升了模型灵活性和表达能力。\n",
    "  \n",
    "- **归一化方式**：  \n",
    "  - Glow 引入 Actnorm 层来代替 BatchNorm，使得在小批量数据下也能稳定训练。\n",
    "\n",
    "- **多尺度架构**：  \n",
    "  - Glow 在多个层次上拆分数据，能更好地处理高分辨率图像，而 Real NVP 在这方面的设计较为简单。\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5. 总结\n",
    "\n",
    "Glow 模型在 Flow-based 生成模型中提出了两大关键改进：\n",
    "1. **可逆 1×1 卷积**：通过学习一个可逆的卷积矩阵来对通道进行混合，相比固定排列操作具有更强的表达能力和灵活性，同时其雅可比行列式易于计算。\n",
    "2. **Actnorm 层**：对数据进行归一化，提高了训练的稳定性。\n",
    "\n",
    "结合仿射耦合层和多尺度架构，Glow 能够构建一个端到端训练的生成模型，既可以精确计算对数似然，又能生成高质量的图像样本。\n",
    "\n",
    "如果你希望进一步实现 Glow，可以考虑参考以下几点：\n",
    "- 理解并实现每个模块（Actnorm、可逆 1×1 卷积、仿射耦合层）的具体数学运算与逆向计算。\n",
    "- 利用 PyTorch 等深度学习框架，通过自动微分实现整个模型的前向传播和梯度反传。\n",
    "- 在实际数据集上进行实验，观察模型在密度估计和采样质量上的表现。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4104ff3c449c59e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.Autoregressive Flow\n",
    "\n",
    "下面详细介绍一下 **Autoregressive Flow**（自回归流）的原理、实现方式以及它与一般的 Normalizing Flow 的区别和联系。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1. Normalizing Flow 的基本概念\n",
    "\n",
    "**Normalizing Flow（规范化流）** 是一种生成模型，其核心思想是利用一系列**可逆变换**（invertible transformations）将一个简单、易采样的分布（如标准正态分布）映射到一个复杂的数据分布上。利用变量替换定理，可以精确计算经过变换后的数据密度。  \n",
    "其基本公式为：  \n",
    "\\[\n",
    "x = f(z), \\quad \\text{且} \\quad p_X(x) = p_Z(z) \\left|\\det \\frac{\\partial f^{-1}(x)}{\\partial x}\\right|,\n",
    "\\]\n",
    "其中 \\(f\\) 通常由多个可逆层构成，而计算雅可比行列式（Jacobian determinant）的方式是设计这些层的关键所在。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2. Autoregressive Flow 的基本思想\n",
    "\n",
    "**Autoregressive Flow** 是 Normalizing Flow 的一个子类，其核心在于**自回归结构**。它要求输出的每个维度只依赖于之前（或者说已知）的维度，换句话说，自回归流满足下列条件：\n",
    "\n",
    "- 给定一个向量 $\\(x = (x_1, x_2, \\dots, x_D)\\)$，自回归模型将联合概率分解为条件概率的乘积：\n",
    "  $\\[\n",
    "  p(x) = \\prod_{i=1}^{D} p(x_i \\mid x_1, x_2, \\dots, x_{i-1}).\n",
    "  \\]$\n",
    "  \n",
    "- 在构造变换 $\\( f \\)$ 时，设计每个维度的映射时只依赖于前面的维度。这种设计通常使用**自回归神经网络**来实现，例如利用掩蔽机制（masking）确保依赖关系仅出现在先前维度上。\n",
    "\n",
    "常见的 Autoregressive Flow 模型包括：  \n",
    "- **MAF（Masked Autoregressive Flow）**：主要用于密度估计，其变换设计使得逆变换（计算生成样本时）需要逐个维度依次计算，但正向计算（密度计算）可以并行化。  \n",
    "- **IAF（Inverse Autoregressive Flow）**：主要用于生成（采样）速度较快的场景，其变换结构和 MAF 相反，正向采样可以并行，而逆向密度计算需要自回归求解。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3. Autoregressive Flow 的实现原理\n",
    "\n",
    "### 4.3.1 自回归建模\n",
    "\n",
    "在 Autoregressive Flow 中，假设我们要构造一个变换 $\\( x = f(z) \\)$，为了满足自回归条件，我们通常设计函数 $\\( f \\)$ 的每个输出维度满足：\n",
    "$\\[\n",
    "x_i = f_i(z_1, z_2, \\dots, z_i; \\theta),\n",
    "\\]$\n",
    "其中 $\\( \\theta \\)$ 为模型参数。这种设计确保在计算雅可比矩阵时，得到的矩阵通常是**下三角矩阵**，其行列式正好是对角线上元素的乘积，这大大简化了雅可比行列式的计算。\n",
    "\n",
    "### 4.3.2 掩蔽技术\n",
    "\n",
    "为了保证每个维度的计算只依赖于之前的维度，常用的方法是**掩蔽（masking）**。例如，在 MADE（Masked Autoencoder for Distribution Estimation）中，通过对网络的连接施加掩码，使得输出 $\\( x_i \\)$ 仅与输入中先于 $\\( i \\)$ 的部分有关。  \n",
    "这种技术被延伸到 Autoregressive Flow 中，确保在进行仿射变换或其他变换时，每个维度只使用了前面的信息。\n",
    "\n",
    "### 4.3.3 变换与雅可比行列式\n",
    "\n",
    "对于自回归流中的每一步变换，由于依赖关系满足自回归性质，其雅可比矩阵通常为下三角形式。假设有变换：\n",
    "$\\[\n",
    "y_i = f_i(x_1, x_2, \\dots, x_i)\n",
    "\\]$\n",
    "那么对应的雅可比矩阵 $\\( J \\)$ 为：\n",
    "$\\[\n",
    "J = \\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & 0 & \\cdots & 0 \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_D}{\\partial x_1} & \\frac{\\partial y_D}{\\partial x_2} & \\cdots & \\frac{\\partial y_D}{\\partial x_D}\n",
    "\\end{pmatrix}.\n",
    "\\]$\n",
    "此时行列式计算为：\n",
    "$\\[\n",
    "\\det J = \\prod_{i=1}^{D} \\frac{\\partial y_i}{\\partial x_i},\n",
    "\\]$\n",
    "只需要计算对角线元素，计算效率高。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4. Autoregressive Flow 与普通 Normalizing Flow 的比较\n",
    "\n",
    "### 4.4.1 共同点\n",
    "\n",
    "- **共同目标**：二者都属于 Normalizing Flow 的范畴，通过一系列可逆变换实现对复杂数据分布的建模。\n",
    "- **最大似然训练**：都可以精确计算数据的对数似然，从而利用最大似然估计进行训练。\n",
    "\n",
    "### 4.4.2 区别\n",
    "\n",
    "- **结构约束**：  \n",
    "  - 普通 Normalizing Flow 不一定要求每个维度之间有自回归依赖，可以设计各种耦合层（如 Real NVP 中的仿射耦合层），这些结构通常允许部分并行计算。  \n",
    "  - Autoregressive Flow 则强制每个维度的输出依赖于前面的所有维度，结构上是严格自回归的。\n",
    "\n",
    "- **计算效率与并行性**：  \n",
    "  - 自回归流的**正向密度计算**可以并行化（例如 MAF 可以同时计算所有条件概率），但在**采样时**通常需要逐个维度依次生成（例如 IAF 则设计为逆向自回归，从而实现快速采样）。  \n",
    "  - 非自回归的耦合层（如 Real NVP、Glow 中的设计）通常允许并行采样，因为各部分可以同时计算。\n",
    "\n",
    "- **模型表达能力**：  \n",
    "  - 自回归流由于每个维度都考虑了之前的所有信息，具有很强的表达能力，能对复杂的条件分布建模。  \n",
    "  - 而其他 Normalizing Flow 模型则依赖于设计特定的耦合层或排列操作，表达能力受限于这些设计的灵活性。\n",
    "\n",
    "- **梯度传递**：  \n",
    "  - 自回归流由于结构上是逐步依赖的，梯度计算时可能会有一定的顺序依赖，设计上需要注意避免梯度消失问题。  \n",
    "  - 非自回归结构的流模型在梯度并行计算上通常更高效。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.5. 总结\n",
    "\n",
    "- **Autoregressive Flow** 是一种特殊形式的 Normalizing Flow，其关键在于利用自回归结构将联合分布拆分成条件分布的乘积，使得每个维度的变换只依赖于前面的维度。这种设计使得雅可比矩阵为下三角矩阵，从而大大简化了行列式的计算。\n",
    "- **与其他 Normalizing Flow**（如基于耦合层的 Real NVP、Glow 等）相比，自回归流在结构上更有序，能捕捉到数据内部的条件依赖关系，但在生成（采样）时可能需要逐步进行，从而影响速度。\n",
    "- **应用场景**：  \n",
    "  - 如果重点在于精确的密度估计和复杂条件分布的建模，Autoregressive Flow 是一个很好的选择；  \n",
    "  - 如果需要快速的并行采样，则可能倾向于使用基于耦合层的流模型。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "562bfc1b9b5c13da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.VAE+Flows\n",
    "下面我详细介绍一下 **VAE + Flows** 的基本思想、数学推导、动机以及如何将其整合到变分自编码器（VAE）中，从而提升近似后验分布的表达能力。\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1. 为什么在 VAE 中使用 Flows？\n",
    "\n",
    "### 5.1.1 标准 VAE 的局限性\n",
    "\n",
    "在标准的 VAE 框架中，我们有两个主要组件：\n",
    "- **编码器（Encoder）**：给定输入 $\\( x \\)$，编码器输出潜变量分布 $\\( q(z|x) \\)$ 的参数，通常假设为多维高斯分布：\n",
    "  $\\[\n",
    "  q(z|x) = \\mathcal{N}(z; \\mu(x), \\operatorname{diag}(\\sigma^2(x))).\n",
    "  \\]$\n",
    "- **解码器（Decoder）**：从潜变量 $\\( z \\)$ 中采样，再通过解码器生成数据 $\\( x \\)$ 的重构。\n",
    "\n",
    "然而，由于实际数据的复杂性，真实的后验 $\\( p(z|x) \\)$ 往往非常复杂，而单一的高斯分布的近似能力有限。这种简单的近似往往称为 **“摊销缺陷”（Amortization Gap）**。\n",
    "\n",
    "### 5.1.2 引入 Flows 改进后验\n",
    "\n",
    "为了获得更灵活、更逼近真实后验的分布，我们可以利用 **Normalizing Flows**。其基本思想是将一个简单的分布（例如上面 VAE 中得到的高斯分布 $\\( q(z|x) \\)$）通过一系列可逆、可微的变换 $\\( f_1, f_2, \\dots, f_K \\)$ 转换为一个更复杂的分布。最终定义的变分后验记为：\n",
    "$\\[\n",
    "z_K = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(z_0), \\quad z_0 \\sim q(z_0|x),\n",
    "\\]$\n",
    "那么经过这组变换后，其密度根据变量替换定理为：\n",
    "$\\[\n",
    "q_K(z_K|x) = q(z_0|x) \\prod_{k=1}^{K} \\left| \\det \\frac{\\partial f_k}{\\partial z_{k-1}} \\right|^{-1}.\n",
    "\\]$\n",
    "这样一来，最终的近似后验 $\\( q_K(z_K|x) \\)$ 就具有比原先单一高斯更高的灵活性和表达能力。\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2. 数学推导与变换公式\n",
    "\n",
    "### 5.2.1 变量替换定理\n",
    "\n",
    "假设有一个可逆变换 $\\( z_1 = f(z_0) \\)$，其密度变换公式为：\n",
    "$\\[\n",
    "q(z_1) = q(z_0) \\left|\\det \\frac{\\partial f^{-1}(z_1)}{\\partial z_1}\\right|.\n",
    "\\]$\n",
    "对于多步变换 $\\( f = f_K \\circ \\cdots \\circ f_1 \\)$ 可得：\n",
    "$\\[\n",
    "q_K(z_K) = q(z_0) \\prod_{k=1}^{K} \\left|\\det \\frac{\\partial f_k^{-1}(z_k)}{\\partial z_k}\\right| = q(z_0) \\prod_{k=1}^{K} \\left|\\det \\frac{\\partial f_k(z_{k-1})}{\\partial z_{k-1}}\\right|^{-1}.\n",
    "\\]$\n",
    "\n",
    "### 5.2.2 将 Flows 整合进 VAE\n",
    "\n",
    "在 VAE 中，我们原先通过编码器直接得到：\n",
    "$\\[\n",
    "q(z|x) = \\mathcal{N}(z; \\mu(x), \\operatorname{diag}(\\sigma^2(x))).\n",
    "\\]$\n",
    "引入 Flow 后，我们把采样得到的 $\\( z_0 \\)$ 进一步变换为 $\\( z_K \\)$：\n",
    "$\\[\n",
    "z_K = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(z_0), \\quad z_0 \\sim \\mathcal{N}(\\mu(x), \\operatorname{diag}(\\sigma^2(x))).\n",
    "\\]$\n",
    "最终使用 $\\( q_K(z_K|x) \\)$ 作为近似后验。因此，整个 VAE 模型的 ELBO（证据下界）可以写为：\n",
    "$\\[\n",
    "\\mathcal{L}(x) = \\mathbb{E}_{z_0 \\sim q(z_0|x)}\\Big[ \\log p(x|z_K) \\Big] - \\operatorname{KL}\\Big(q_K(z_K|x) \\,\\|\\, p(z)\\Big).\n",
    "\\]$\n",
    "其中 $\\( p(z) \\)$ 通常为标准正态分布。\n",
    "\n",
    "注意到 $\\( \\log q_K(z_K|x) \\)$ 可展开为：\n",
    "$\\[\n",
    "\\log q_K(z_K|x) = \\log q(z_0|x) - \\sum_{k=1}^{K} \\log \\left| \\det \\frac{\\partial f_k(z_{k-1})}{\\partial z_{k-1}} \\right|.\n",
    "\\]$\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3. 优化与训练\n",
    "\n",
    "### 5.3.1 损失函数\n",
    "\n",
    "整体损失函数（针对单个样本）为 ELBO：\n",
    "$\\[\n",
    "\\mathcal{L}(x) = \\mathbb{E}_{z_0 \\sim q(z_0|x)} \\left[\\log p(x|z_K) \\right] - \\operatorname{KL}\\left( q(z_0|x) \\,\\Big\\|\\, p(z) \\right) + \\sum_{k=1}^{K} \\log \\left| \\det \\frac{\\partial f_k(z_{k-1})}{\\partial z_{k-1}} \\right|.\n",
    "\\]$\n",
    "（注意：KL 项经过 flow 变换时会加上累积的 log-det 校正项。）\n",
    "\n",
    "### 5.3.2 参数优化\n",
    "\n",
    "- **编码器**：负责输出 $\\( \\mu(x) \\)$ 和 $\\( \\sigma(x) \\)$，其参数通过反向传播更新。\n",
    "- **Flows**：每个变换 $\\( f_k \\)$ 都由神经网络参数化（例如仿射耦合层、可逆卷积等），其参数也在整体梯度下降中更新。\n",
    "- **解码器**：从最终变换后的 $\\( z_K \\)$ 中生成重构数据 $\\( x \\)$。\n",
    "\n",
    "利用自动微分工具（如 PyTorch），整个前向过程（从 $\\( x \\)$ 到 $\\( z_0 \\)$ 再到 $\\( z_K \\)$，并计算累积的雅可比行列式）都是可微分的，最终损失函数对所有参数进行反向传播更新。\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4. 直观理解\n",
    "\n",
    "你可以将 VAE+Flows 理解为一个两阶段过程：\n",
    "1. **编码阶段**：输入 $\\( x \\)$ 经编码器得到一个简单的近似后验（如高斯分布），从中采样得到 $\\( z_0 \\)$。\n",
    "2. **流变换阶段**：对 $\\( z_0 \\)$ 应用一系列精心设计的可逆变换，将其转化为一个更加复杂、灵活的分布 $\\( z_K \\)$；同时通过雅可比行列式对密度进行校正。\n",
    "3. **解码阶段**：将 $\\( z_K \\)$ 输入解码器生成数据 $\\( x \\)$ 的重构。\n",
    "\n",
    "这种方法能够有效克服传统 VAE 中简单高斯后验的限制，使得生成的样本质量更高，并且更好地逼近真实的潜变量分布。\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5. 参考模型与论文\n",
    "\n",
    "一些经典工作：\n",
    "- **Variational Inference with Normalizing Flows** (Rezende and Mohamed, 2015)  \n",
    "  详细介绍了如何将普通的变分推断与 Flow 结合，从而提高后验的灵活性。\n",
    "- **Improving Variational Inference with Inverse Autoregressive Flow** (Kingma et al., 2016)  \n",
    "  提出 IAF 作为一种高效的 flow 方法，能够在采样时并行计算。\n",
    "\n",
    "---\n",
    "\n",
    "## 5.6. 总结\n",
    "\n",
    "**VAE+Flows** 的主要思想就是利用 normalizing flows 来扩展 VAE 中近似后验的表达能力。简单来说：\n",
    "- 编码器给出简单分布（如高斯）的参数；\n",
    "- 通过一系列可逆变换，将这个简单分布变换为更复杂的分布；\n",
    "- 利用变量替换定理校正密度，并计算最终的对数似然；\n",
    "- 最终通过最大似然（或 ELBO）来训练整个模型，所有部分（编码器、流变换、解码器）的参数共同优化。\n",
    "\n",
    "这种方法使得 VAE 能够生成更精细、更符合真实数据分布的样本，同时改善了模型在复杂数据集上的表现。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18f290706ae34bd2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "54842fc97cf7726f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bea72703e6e8fc97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa04d01db38ed91"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
