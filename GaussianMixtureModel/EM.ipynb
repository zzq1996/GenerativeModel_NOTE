{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM算法\n",
    "\n",
    "EM算法（Expectation-Maximization，期望最大化算法）是一种用于在存在隐变量（或缺失数据）的情形下，求解模型参数最大似然估计（MLE）或最大后验估计（MAP）的迭代优化方法。它广泛应用于混合模型（如混合高斯模型）、隐马尔可夫模型等场景。下面将详细介绍EM算法的基本原理、推导过程以及每个步骤的具体含义。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 问题背景\n",
    "\n",
    "在很多实际问题中，我们希望对观测数据 $\\( X = \\{x_1, x_2, \\ldots, x_N\\} \\)$ 建立概率模型，但往往数据生成过程中存在无法直接观测的隐变量 $\\( Z = \\{z_1, z_2, \\ldots, z_N\\} \\)$（例如混合高斯模型中的类别标签）。假设模型的参数为 $\\(\\theta\\)$，则数据的**完全数据似然函数**为：\n",
    "$\\[\n",
    "p(X,Z|\\theta)\n",
    "\\]$\n",
    "而由于隐变量 \\(Z\\) 未观测到，我们只能利用**不完全数据似然函数**：\n",
    "$\\[\n",
    "p(X|\\theta) = \\sum_Z p(X,Z|\\theta)\n",
    "\\]$\n",
    "直接最大化 $\\(p(X|\\theta)\\)$ 或其对数似然 $\\(\\log p(X|\\theta)\\)$ 往往十分困难，原因在于对数内部存在求和，使得求导和求极值问题变得复杂。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. EM算法的基本思想\n",
    "\n",
    "EM算法的核心思想是利用**交替优化**的思想，将直接求解不完全数据的对数似然问题转化为两个较易处理的子问题：\n",
    "\n",
    "1. **E步（Expectation Step，期望步骤）**：在当前参数估计 $\\(\\theta^{(t)}\\)$ 下，计算隐变量 $\\(Z\\)$ 的后验分布 $\\(p(Z|X,\\theta^{(t)})\\)$，并求出完全数据对数似然的期望，即构造一个下界函数 $\\(Q\\)$：\n",
    "   $\\[\n",
    "   Q(\\theta|\\theta^{(t)}) = \\mathbb{E}_{Z|X,\\theta^{(t)}}\\left[\\log p(X,Z|\\theta)\\right]\n",
    "   \\]$\n",
    "   直观地说，E步利用当前参数对缺失信息进行“填补”，为后续的参数更新提供依据。\n",
    "\n",
    "2. **M步（Maximization Step，最大化步骤）**：在E步构造的函数 \\(Q(\\theta|\\theta^{(t)})\\) 的基础上，对参数 \\(\\theta\\) 进行最大化更新：\n",
    "   \\[\n",
    "   \\theta^{(t+1)} = \\arg\\max_\\theta Q(\\theta|\\theta^{(t)})\n",
    "   \\]\n",
    "   通过这一步，我们得到一组使得下界最大的参数更新，从而提高了观测数据的似然值。\n",
    "\n",
    "这两个步骤不断交替进行，直到参数收敛（例如对数似然的变化小于某个阈值）。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. EM算法的数学推导\n",
    "\n",
    "### 3.1 利用Jensen不等式构造下界\n",
    "\n",
    "考虑对数似然：\n",
    "\\[\n",
    "\\log p(X|\\theta) = \\log \\sum_Z p(X,Z|\\theta)\n",
    "\\]\n",
    "引入任意的概率分布 \\(q(Z)\\)（满足 \\(\\sum_Z q(Z)=1\\) 且 \\(q(Z) > 0\\)），有：\n",
    "\\[\n",
    "\\log p(X|\\theta) = \\log \\sum_Z q(Z) \\frac{p(X,Z|\\theta)}{q(Z)}\n",
    "\\]\n",
    "利用Jensen不等式（对于凹函数，\\(\\log \\mathbb{E}[Y] \\geq \\mathbb{E}[\\log Y]\\)），可以得到：\n",
    "\\[\n",
    "\\log p(X|\\theta) \\geq \\sum_Z q(Z) \\log \\frac{p(X,Z|\\theta)}{q(Z)}\n",
    "\\]\n",
    "记这个下界为：\n",
    "\\[\n",
    "\\mathcal{L}(q,\\theta) = \\sum_Z q(Z) \\log p(X,Z|\\theta) - \\sum_Z q(Z) \\log q(Z)\n",
    "\\]\n",
    "可以证明，当我们取 \\(q(Z) = p(Z|X,\\theta^{(t)})\\) 时，这个下界对 \\(\\theta\\) 最为紧密，此时：\n",
    "\\[\n",
    "Q(\\theta|\\theta^{(t)}) = \\mathbb{E}_{Z|X,\\theta^{(t)}}\\left[\\log p(X,Z|\\theta)\\right]\n",
    "\\]\n",
    "而第二项（熵项）与 \\(\\theta\\) 无关，因此在M步只需最大化 \\(Q\\) 函数。\n",
    "\n",
    "### 3.2 E步的详细内容\n",
    "\n",
    "在第 \\(t\\) 次迭代中，给定当前参数 \\(\\theta^{(t)}\\)，计算每个可能的隐变量配置 \\(Z\\) 的后验概率：\n",
    "\\[\n",
    "q(Z) = p(Z|X,\\theta^{(t)})\n",
    "\\]\n",
    "然后计算期望：\n",
    "\\[\n",
    "Q(\\theta|\\theta^{(t)}) = \\sum_Z p(Z|X,\\theta^{(t)}) \\log p(X,Z|\\theta)\n",
    "\\]\n",
    "对于连续型隐变量，求和可以替换为积分。这一步实际上是在“软填补”缺失数据，为参数更新提供充分信息。\n",
    "\n",
    "### 3.3 M步的详细内容\n",
    "\n",
    "在M步中，固定 \\(Q(\\theta|\\theta^{(t)})\\) 的表达形式，对参数 \\(\\theta\\) 进行最大化：\n",
    "\\[\n",
    "\\theta^{(t+1)} = \\arg\\max_\\theta Q(\\theta|\\theta^{(t)})\n",
    "\\]\n",
    "这一步往往比直接最大化 \\(\\log p(X|\\theta)\\) 要简单，因为完全数据的对数似然 \\(\\log p(X,Z|\\theta)\\) 通常具有较简单的形式（例如对数似然分解成各部分参数独立的和）。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. EM算法的性质\n",
    "\n",
    "- **单调性**：EM算法保证每次迭代后，观测数据对数似然 \\(\\log p(X|\\theta)\\) 都不会减小。这是由于：\n",
    "  \\[\n",
    "  \\log p(X|\\theta^{(t+1)}) \\geq \\mathcal{L}(q,\\theta^{(t+1)}) \\geq \\mathcal{L}(q,\\theta^{(t)}) = \\log p(X|\\theta^{(t)})\n",
    "  \\]\n",
    "  因此，EM算法的迭代过程总是沿着似然值上升的方向前进，直至收敛到局部极大值（或鞍点）。\n",
    "\n",
    "- **局部最优性**：EM算法只能保证收敛到局部最优解或鞍点，因此参数的初始化对结果有较大影响。\n",
    "\n",
    "- **适用范围**：只要能够计算出 \\(p(Z|X,\\theta^{(t)})\\) 和 \\(Q(\\theta|\\theta^{(t)})\\) 的解析形式，EM算法就可以应用；在很多模型中，如混合模型和隐马尔可夫模型中，都能方便地应用EM算法。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 以混合高斯模型为例\n",
    "\n",
    "在混合高斯模型中：\n",
    "- **隐变量 \\(Z\\)**：表示每个数据点 \\(x_i\\) 属于哪一个高斯成分，其取值 \\(z_i \\in \\{1,2,\\dots,K\\}\\)。\n",
    "- **完全数据似然**：\n",
    "  \\[\n",
    "  p(x_i, z_i=k|\\theta) = \\pi_k\\, \\mathcal{N}(x_i|\\mu_k, \\Sigma_k)\n",
    "  \\]\n",
    "  其中 \\(\\theta = \\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^K\\)。\n",
    "\n",
    "**E步**：\n",
    "计算每个数据点属于每个成分的后验概率（责任）：\n",
    "\\[\n",
    "\\gamma(z_{ik}) = P(z_i=k|x_i,\\theta^{(t)}) = \\frac{\\pi_k^{(t)}\\,\\mathcal{N}(x_i|\\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)}\\,\\mathcal{N}(x_i|\\mu_j^{(t)}, \\Sigma_j^{(t)})}\n",
    "\\]\n",
    "这相当于利用当前参数对隐变量 \\(z_i\\) 进行“软分配”。\n",
    "\n",
    "**M步**：\n",
    "利用E步得到的责任 \\(\\gamma(z_{ik})\\) 更新参数：\n",
    "- 更新混合系数：\n",
    "  \\[\n",
    "  \\pi_k^{(t+1)} = \\frac{1}{N}\\sum_{i=1}^N \\gamma(z_{ik})\n",
    "  \\]\n",
    "- 更新均值：\n",
    "  \\[\n",
    "  \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^N \\gamma(z_{ik})\\, x_i}{\\sum_{i=1}^N \\gamma(z_{ik})}\n",
    "  \\]\n",
    "- 更新协方差矩阵：\n",
    "  \\[\n",
    "  \\Sigma_k^{(t+1)} = \\frac{\\sum_{i=1}^N \\gamma(z_{ik})\\, (x_i-\\mu_k^{(t+1)})(x_i-\\mu_k^{(t+1)})^T}{\\sum_{i=1}^N \\gamma(z_{ik})}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "EM算法通过以下两个交替步骤实现参数估计：\n",
    "1. **E步**：利用当前参数估计，计算隐变量的后验分布，为每个数据点“填补”缺失信息。\n",
    "2. **M步**：在填补了隐变量的基础上，最大化完全数据对数似然的期望，从而更新参数。\n",
    "\n",
    "这种方法使得即使在存在缺失数据或隐变量的情况下，也能通过分步优化的方式近似求解最大似然问题。尽管EM算法可能收敛到局部最优解，并且依赖于初始参数，但其简单高效的结构使其在统计学习和机器学习中得到了广泛应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入numpy库 \n",
    "import numpy as np\n",
    "\n",
    "### EM算法过程函数定义\n",
    "def em(data, thetas, max_iter=30, eps=1e-3):\n",
    "    '''\n",
    "    输入：\n",
    "    data：观测数据\n",
    "    thetas：初始化的估计参数值\n",
    "    max_iter：最大迭代次数\n",
    "    eps：收敛阈值\n",
    "    输出：\n",
    "    thetas：估计参数\n",
    "    '''\n",
    "    # 初始化似然函数值\n",
    "    ll_old = -np.infty\n",
    "    for i in range(max_iter):\n",
    "        ### E步：求隐变量分布\n",
    "        # 对数似然\n",
    "        log_like = np.array([np.sum(data * np.log(theta), axis=1) for theta in thetas])\n",
    "        # 似然\n",
    "        like = np.exp(log_like)\n",
    "        # 求隐变量分布\n",
    "        ws = like/like.sum(0)\n",
    "        # 概率加权\n",
    "        vs = np.array([w[:, None] * data for w in ws])\n",
    "        ### M步：更新参数值\n",
    "        thetas = np.array([v.sum(0)/v.sum() for v in vs])\n",
    "        # 更新似然函数\n",
    "        ll_new = np.sum([w*l for w, l in zip(ws, log_like)])\n",
    "        print(\"Iteration: %d\" % (i+1))\n",
    "        print(\"theta_B = %.2f, theta_C = %.2f, ll = %.2f\" \n",
    "              % (thetas[0,0], thetas[1,0], ll_new))\n",
    "        # 满足迭代条件即退出迭代\n",
    "        if np.abs(ll_new - ll_old) < eps:\n",
    "            break\n",
    "        ll_old = ll_new\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "theta_B = 0.71, theta_C = 0.58, ll = -32.69\n",
      "Iteration: 2\n",
      "theta_B = 0.75, theta_C = 0.57, ll = -31.26\n",
      "Iteration: 3\n",
      "theta_B = 0.77, theta_C = 0.55, ll = -30.76\n",
      "Iteration: 4\n",
      "theta_B = 0.78, theta_C = 0.53, ll = -30.33\n",
      "Iteration: 5\n",
      "theta_B = 0.79, theta_C = 0.53, ll = -30.07\n",
      "Iteration: 6\n",
      "theta_B = 0.79, theta_C = 0.52, ll = -29.95\n",
      "Iteration: 7\n",
      "theta_B = 0.80, theta_C = 0.52, ll = -29.90\n",
      "Iteration: 8\n",
      "theta_B = 0.80, theta_C = 0.52, ll = -29.88\n",
      "Iteration: 9\n",
      "theta_B = 0.80, theta_C = 0.52, ll = -29.87\n",
      "Iteration: 10\n",
      "theta_B = 0.80, theta_C = 0.52, ll = -29.87\n",
      "Iteration: 11\n",
      "theta_B = 0.80, theta_C = 0.52, ll = -29.87\n",
      "Iteration: 12\n",
      "theta_B = 0.80, theta_C = 0.52, ll = -29.87\n"
     ]
    }
   ],
   "source": [
    "# 观测数据，5次独立试验，每次试验10次抛掷的正反次数\n",
    "# 比如第一次试验为5次正面5次反面\n",
    "observed_data = np.array([(5,5), (9,1), (8,2), (4,6), (7,3)])\n",
    "# 初始化参数值，即硬币B的正面概率为0.6，硬币C的正面概率为0.5\n",
    "thetas = np.array([[0.6, 0.4], [0.5, 0.5]])\n",
    "thetas = em(observed_data, thetas, max_iter=30, eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7967829 , 0.2032171 ],\n",
       "       [0.51959543, 0.48040457]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
